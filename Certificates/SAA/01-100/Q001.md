## Q001. S3

**📄 문제**  
회사는 여러 대륙에 걸쳐 도시의 온도, 습도 및 대기압에 대한 데이터를 수집합니다. 
회사가 매일 각 사이트에서 수집하는 데이터의 평균 볼륨은 500GB 입니다. 각 사이트에는 고속 인터넷 연결이 있습니다. 
이 회사는 이러한 모든 글로벌 사이트의 데이터를 단일 Amazon S3 버킷에 최대한 빨리 집계하려고 합니다. 솔루션은 운영 복잡성을 최소화해야 합니다. 
어떤 솔루션이 이러한 요구 사항을 충족합니까? 

**📝 선택지**
- A. 대상 S3 버킷에서 S3 Transfer Acceleration 을 켭니다. 멀티파트 업로드를 사용하여 사이트 데이터를 대상 S3 버킷에 접 업로드합니다. 
- B. 각 사이트의 데이터를 가장 가까운 리전의 S3 버킷에 업로드합니다. S3 교차 리전 복제를 사용하여 대상 S3 버킷에 객체를 복사합니다. 그런 다음 원본 S3 버킷에서 데이터를 제거합니다. 
- C. AWS Snowball Edge Storage Optimized 디바이스 작업을 매일 예약하여 각 사이트에서 가장 가까운 리전으로 데이터를 전송합니다. S3 교차 리전 복제를 사용하여 대상 S3 버킷에 객체를 복사합니다.
- D. 각 사이트의 데이터를 가장 가까운 리전의 Amazon EC2 인스턴스로 업로드합니다. Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를 저장합니다. 정기적으로 EBS 스냅샷을 만들어 대상 S3 버킷이 포함된 리전에 복사합니다. 해당 리전에서 EBS 볼륨을 복원합니다. 

**✅ 정답:** A 

**📚 해설**
- **제공 해설**
여러 글로벌 사이트의 데이터를 단일 S3 버킷에 최대한 빨리 집계하는 동시에 운영 복잡성을 최소화 하기 위해서는 A이다. S3 Transfer Acceleration은 여러 글로벌 사이트의 데이터를 단일 S3로 신속하게 집계하는 가장 효율적이고 운영상 간단한 솔루션이다. 이는 멀티 파트 업로드를 활용해서 빠른 데이터 수집을 단순하게 할 수 있다. 
- **왜 맞는지:**  
  S3 Transfer Acceleration은 빠르게 Edge Location으로 빠르게 public www통해서 업로드를 한 후, 이를 S3 bucket으로 target region까지 빠르게 업로드하도록 한다. 이때, multi part upload와 함께 이를 할 수 있다. public network 사용을 최소화하고, private network 사용을 극대화 하도록 한다. Multipart upload는 100MB 이상의 파일들은 추천되고, **5GB** 이상의 파일들은 무조건 multipart upload를 실행해야 한다. 병렬 업로드를 통해서 더 빠르게 업로드 할 수 있다. 
  - 고속 인터넷, 글로벌 데이터를 -> 단일 S3 Bucket, 
- **왜 아닌지:**  
  - B: CRR(Cross Region Replication)과 SRR(Same-region Replication)이 있음. 이 둘은 다른 AWS 계정에 있어도 되고, 비동기적 복사이며, IAM 권한이 잘 부여 되어야 해.
  CRR는 compliance, lower latency access, 그리고 replication across accounts가 가능하고, SRR는 log aggregation, live replication between prod and test accoutns로 사용 됨. 근데 이건 가능은 하지만, 굳이임. 
  - C: Snowball은 USB와 비슷한 이동 디바이스로, edge에서 데이터를 모은 후, 이를 AWS 안이나 밖으로 이동하도록 하는 서비스. 이건 network을 통해서 1주일 넘게 걸리는 대용량, 멀리 있는 데이터인 경우, snowball에 담아서 shipping. 
  - D: 불필요한 ? 

**🔑 키워드 / 태그**
`#S3`
